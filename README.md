# Experiment with Optimizers

## Description

This project focuses on conducting an experiment using four different optimizers: Adam, SGD, Adagrad, and RMSprop. 
All optimizers were tested on the same model with the same data to evaluate their performance and efficiency.
Model type: CNN
Dataset: https://www.kaggle.com/datasets/mahmoudreda55/satellite-image-classification

## Optimizers

1. **Adam**: An adaptive gradient descent method that uses first and second-order moments to update the weights.
2. **SGD (Stochastic Gradient Descent)**: A standard gradient descent method that updates model weights based on random subsets of the data.
3. **Adagrad**: An adaptive method that adjusts the learning rate for each parameter based on its gradient history.
4. **RMSprop**: An adaptive method that also adjusts the learning rate, considering the average of the squared gradients.
   
*To be continued in the future with other possible optimization algorithms...*

## Running the Experiment

1. Clone the repository:
   ```bash
   git clone https://github.com/Maks6666/optimization_experiment.git
2. You may just upload weight of models with performed optimizers and try them by yourself.

**All notes and conclussions will be available in exp1.ipynb file.**
